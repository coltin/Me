Textbook: Programming Massively Parallel Processors by David B. Kirk and Wen-mei W. Hwu
Chapters to read:
 1, 3, 4.1, 4.2, 4.3

TEXTBOOK NOTES

LECTURE NOTES

1.2
===
Latency Devices (CPU Cores)
 - Larger cache, smaller number of registers, fewer SIMD Units, sophisticated control logic
 - Powerful ALU - Reduced operation latency (Arithmatic logic units)
 - Sophisticated control - Branch prediction for reduced branch latency, data forwarding for reduced data latency [????]
 - CPUs can be 10+X faster than GPUs for sequential code.
Throughput Devices (GPU Cores)
 - Simple control, LARGE number of threads to manage
 - Small cache, to boost memory throughput
 - Simple control. No branch prediction, no data forwarding.
 - Energy efficient ALUs - Many ong latency but heavily pipelined for high throughput.
 - Massive number of threads to tolerate latencies.
 - GPUs can be 10+X faster than CPUs for parallel code.
SOC = System on a Chip
HW IP Cores
DSP Cores - Digital Signature Processing Cores
On Chip Memories
Cloud Services

1.3
===
SW lines per chip increases 2x every 10 months
HW gates per chip increases 2x every 18 months

1.4
===
ISA is Instruction Set Architecture, a contract beween the hardware and software.
 -> Set of instructions that the hardware can execute.
Blocks and Threads can be 1D, 2D, or 3D. (z, y, x), which is reverse of what you'd think.

1.5
===
Need "#include <cuda.h>" to use cuda code.

void vectorAdd(float* h_A, float* h_B, float* h_C, int n) {
	int size = n * sizeof(float);
	float* d_A, d_B, d_C;

	// 1. Allocate device memory for A, B, and C.
	// Copy A and B to device memory.

	// cudaError_t error = cudaMalloc((void **) &d_A, size);

	if (cudaMalloc((void **) &d_A, size) != cudaSuccess) {
		printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);
		exit(EXIT_FAILURE);
	}
	cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);

	if (cudaMalloc((void **) &d_B, size) != cudaSuccess) {
		printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);
		exit(EXIT_FAILURE);
	}
	cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

	if (cudaMalloc((void **) &d_A, size) != cudaSuccess) {
		printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);
		exit(EXIT_FAILURE);
	}

	// 2. Kernel launch code - the device performs the actual vector addition.

	// 3. copy C from the device memory.

	cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
	cudaFree(d_A);
	cudaFree(d_B);
	cudaFree(d_C);
}

cudaMalloc()
	Allocates object in the device global memory.
	Two params. Address of pointer to the allocated object. Size of allocated object in bytes.
cudaFree()
	Frees object from global memory. Returns pointer to freed object.
cudaMemcpy()
	memory data transfer
	Requires four parameters
		Pointer to destination, pointer to source, number of bytes to copy, type/direction of transfer (predefined constants)
	Asynchronous

1.6
===
__global__
void kernelVectorAdd(float* A, float* B, float* C, int n) {
	int i = threadIdx.x + blockDim.x*blockIdx.x;
	if (i < n) {
		return;
	}
	C[i] = A[i] + B[i];
}

__host__
void vectorAdd(float* h_A, float* h_B, float* h_C, int n) {
	// allocations and copies
	// Run ceil (n/256.0) blocks of 256 threads each
	vecAddKernnel<<<ceil(n/256.0), 256>>>(d_A, d_B, d_C, n);
}

// For multiple dimensions.
__host__
void vectorAdd(float* h_A, float* h_B, float* h_C, int n) {
	// x, y, z. dim3 is defined in "cude.h"
	dim3 DimGrid((n-1) / 256 + 1, 1, 1, 1);
	dim3 DimBlock(256, 1, 1);
	vecAddKernnel<<<DimGrid, DimBlock>>>(d_A, d_B, d_C, n);
}
                                  Exectured on         Callable from
__device__ float DeviceFunc()        device                device
__global__ void  KernelFunc()        device                 host
__host__   float HostFunc()           host                  host

__global__ must return void.
__device__ and __host__ can be used at the same time! To create utility methods. It will be compiled to host and device functions.

Compiler splits these into TWO code paths.
Host compiles like normal.
Device compiles to Device Code (PTX) and a JIT will compile it at runtime to ISA (Instruction set architecture)

1.7
===
Multi-dimensional block and thread indices.
Mapping block/thread indices to data indices.

62x76 = 62 rows, 76 columns = (y,x)
16 by 16 threads per block
So 4 y thread blocks
   5 x thread blocks

Memory in C is row major.
M0,0 M0,1 M0,2
M1,0 M1,1 M1,2
M2,0 M2,1 M2,2
Row*Width + column = linear address space

__global__ void PictureKernel(float* d_Pin, ...) {
	int Row = blockIdx.y * blockDim.y + threadIdx.y;
	int Col = blockIdx.x * blockDim.x + threadIdx.x;
	if ((Row >= nRow) || (Col >= nCol)) {
		return;
	}
	int linearAddress = Row*nRow + Col;
	d_Pout[linearAddress] = 2.0 * d_Pin[linearAddress]
}

__host__ void launchKernel() {
	// Assume picture is mxn.
	// m pixels in y dimensions, and n pixels in x dimension
	// input d_Pin has been allocated on and copied to device
	// output d_Pout has been allocated on device
	dim3 DimGrid((n-1)/16 + 1, ((m-1)/16 + 1), 1);
	dim3 DimBlock(16, 16, 1);
	PictureKernel<<<DimGrid, DimBlock>>>(d_Pin, d_Pout, m, n); // Drives me nuts that they do row major... (y,x) feels so wrong.
}

1.8
===
Quick review of matrix multiplication
Block/Thread index to data index mapping
Loop control flow in kernels

[Row, Col] is generated by taking the inner product of a row with A (using index Row) and column B (with index Col)
A_(m,n)
B_(n,k)
C_(m,k) = AxB

// In CPU Host code
void MatrixMultiplicationOnHost(int m, int n, int k, float* A, float* B, float* C) {
	for (int row = 0; row < m; row++) {
		for (int col = 0; col < k; col++) {
			float sum = 0;
			for (int i = 0; i < n; i++) {
				float a = A[row*n + i];
				float b = B[col + k*i];
				sum += a*b;
			}
			C[row*k + col] = sum;
		}
	}
}

For 

// Assume TILE_WIDTH is a #define constant of 2
__host__
void MatrixMultiplication(int m, int n, int k, float* d_A, float* d_B, float* d_C) {
	dim3 dimGrid((k-1) / TILE_WIDTH + 1, (m-1) / TILE_WIDTH + 1, 1);
	dim3 dimBlock(TILE_WIDTH, TILE_WIDTH, 1);
	KernelMatrixMultiplication<dimGrid, dimBlock>>>(m, n, k, d_A, d_B, d_C);
}

__global void KernelMatrixMultiplication(int m, int n, int k, float* d_A, float* d_B, float* d_C) {
	int row = blockIdx.y * blockDim.y + threadIdx.y;
	int col = blockIdx.x * blockDim.x + threadIdx.x;
	if (row >= m) || (col >= k) {
		return;
	}

	float cValue = 0.0;
	for (int i = 0; i < n; i++) {
		cValue += A[row*n + i] * B[col + i*k];
	}
	C[row*k + col] = cValue;
}




