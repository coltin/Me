Textbook: Programming Massively Parallel Processors by David B. Kirk and Wen-mei W. Hwu
Chapters to read:
 1, 3, 4.1, 4.2, 4.3

TEXTBOOK NOTES

Chapter 3
cudaMemcpy(destination, source, memorySizeBytes, DIRECTION)
	Cannot be used to copy memory between seperate GPU Devices.
	DIRECTIONS:
		cudaMemcpyHostToDevice
		cudaMemcpyDeviceToHost
		cudaMemcpyDeviceToDevice
		cudaMemcpyHostToHost
SPMD - Single Program, Multiple-Data
	A popular parallel programming style.
Executing kernel methods.
	FunctionName<<<dimensionOfBlocks, dimensionOfThreads>>>(function_params);
	dimensionOfBlocks, also dimGrid by the textbook, defines how many blocks there are.
	dimensionOfThreads, also dimBlock by the textbook, defines how many threads there are in each block.

The "CUDA Programming Guide" is a thing. I should probably get that.

Chapter 4
Blocks and threads can be specified in up to three dimensions, and the index can be accessed like so:
	blockIdx.x, blockIdx.y, blockIdx.z
	threadIdx.x, threadIdx.y, threadIdx.z
gridDim provide the dimension of the grid. Usage: ???
blockDim dimension of each block. Usage: blockDim.x, blockDim.y, blockDim.z
int threadId = blockIdx.x * blockDim.x + threadIdx.x;

For 1D blocks or threads, can use <<<int numberOfBlocks, int numberOfThreads>>>.
In contrast to <<<dim3 blockDimension, dim3 threadDimension>>>

Criticism: The book says things like "In general, blocks are organized into 3D arrays of threads." but doesn't justify why this is true.

Syncronization
	__syncthreads() will block this thread until every single thread in the block reaches this point.
		Importance of __syncthreads() will be discussed in Chapter 5. Excited to read that! YES :D

RANDOM NOTES
methodName defines a method that is on the host, callable from the host. ie, __host__
kMethodName defines a method that is in the kernel, callable from the Host. ie, __global__
dMethodName defines a method that is in the device, callable from the device. ie, __device__
uMethodName defines a utility. This is a "utility" that methods on the Host/Device can use. ie, __host__ __device__
dVariable defines a variable that points to device memory.

LECTURE NOTES

1.2
===
Latency Devices (CPU Cores)
 - Larger cache, smaller number of registers, fewer SIMD Units, sophisticated control logic
 - Powerful ALU - Reduced operation latency (Arithmatic logic units)
 - Sophisticated control - Branch prediction for reduced branch latency, data forwarding for reduced data latency [????]
 - CPUs can be 10+X faster than GPUs for sequential code.
Throughput Devices (GPU Cores)
 - Simple control, LARGE number of threads to manage
 - Small cache, to boost memory throughput
 - Simple control. No branch prediction, no data forwarding.
 - Energy efficient ALUs - Many ong latency but heavily pipelined for high throughput.
 - Massive number of threads to tolerate latencies.
 - GPUs can be 10+X faster than CPUs for parallel code.
SOC = System on a Chip
HW IP Cores
DSP Cores - Digital Signature Processing Cores
On Chip Memories
Cloud Services

1.3
===
SW lines per chip increases 2x every 10 months
HW gates per chip increases 2x every 18 months

1.4
===
ISA is Instruction Set Architecture, a contract beween the hardware and software.
 -> Set of instructions that the hardware can execute.
Blocks and Threads can be 1D, 2D, or 3D. (z, y, x), which is reverse of what you'd think.

1.5
===
Need "#include <cuda.h>" to use cuda code.

void vectorAdd(float* h_A, float* h_B, float* h_C, int n) {
	int size = n * sizeof(float);
	float* d_A, d_B, d_C;

	// 1. Allocate device memory for A, B, and C.
	// Copy A and B to device memory.

	// cudaError_t error = cudaMalloc((void **) &d_A, size);

	if (cudaMalloc((void **) &d_A, size) != cudaSuccess) {
		printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);
		exit(EXIT_FAILURE);
	}
	cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);

	if (cudaMalloc((void **) &d_B, size) != cudaSuccess) {
		printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);
		exit(EXIT_FAILURE);
	}
	cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

	if (cudaMalloc((void **) &d_A, size) != cudaSuccess) {
		printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);
		exit(EXIT_FAILURE);
	}

	// 2. Kernel launch code - the device performs the actual vector addition.

	// 3. copy C from the device memory.

	cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
	cudaFree(d_A);
	cudaFree(d_B);
	cudaFree(d_C);
}

cudaMalloc()
	Allocates object in the device global memory.
	Two params. Address of pointer to the allocated object. Size of allocated object in bytes.
cudaFree()
	Frees object from global memory. Returns pointer to freed object.
cudaMemcpy()
	memory data transfer
	Requires four parameters
		Pointer to destination, pointer to source, number of bytes to copy, type/direction of transfer (predefined constants)
	Asynchronous

1.6
===
__global__
void kernelVectorAdd(float* A, float* B, float* C, int n) {
	int i = threadIdx.x + blockDim.x*blockIdx.x;
	if (i < n) {
		return;
	}
	C[i] = A[i] + B[i];
}

__host__
void vectorAdd(float* h_A, float* h_B, float* h_C, int n) {
	// allocations and copies
	// Run ceil (n/256.0) blocks of 256 threads each
	vecAddKernnel<<<ceil(n/256.0), 256>>>(d_A, d_B, d_C, n);
}

// For multiple dimensions.
__host__
void vectorAdd(float* h_A, float* h_B, float* h_C, int n) {
	// x, y, z. dim3 is defined in "cude.h"
	dim3 DimGrid((n-1) / 256 + 1, 1, 1, 1);
	dim3 DimBlock(256, 1, 1);
	vecAddKernnel<<<DimGrid, DimBlock>>>(d_A, d_B, d_C, n);
}
                                  Exectured on         Callable from
__device__ float DeviceFunc()        device                device
__global__ void  KernelFunc()        device                 host
__host__   float HostFunc()           host                  host

__global__ must return void.
__device__ and __host__ can be used at the same time! To create utility methods. It will be compiled to host and device functions.

Compiler splits these into TWO code paths.
Host compiles like normal.
Device compiles to Device Code (PTX) and a JIT will compile it at runtime to ISA (Instruction set architecture)

1.7
===
Multi-dimensional block and thread indices.
Mapping block/thread indices to data indices.

62x76 = 62 rows, 76 columns = (y,x)
16 by 16 threads per block
So 4 y thread blocks
   5 x thread blocks

Memory in C is row major.
M0,0 M0,1 M0,2
M1,0 M1,1 M1,2
M2,0 M2,1 M2,2
Row*Width + column = linear address space

__global__ void PictureKernel(float* d_Pin, ...) {
	int Row = blockIdx.y * blockDim.y + threadIdx.y;
	int Col = blockIdx.x * blockDim.x + threadIdx.x;
	if ((Row >= nRow) || (Col >= nCol)) {
		return;
	}
	int linearAddress = Row*nRow + Col;
	d_Pout[linearAddress] = 2.0 * d_Pin[linearAddress]
}

__host__ void launchKernel() {
	// Assume picture is mxn.
	// m pixels in y dimensions, and n pixels in x dimension
	// input d_Pin has been allocated on and copied to device
	// output d_Pout has been allocated on device
	dim3 DimGrid((n-1)/16 + 1, ((m-1)/16 + 1), 1);
	dim3 DimBlock(16, 16, 1);
	PictureKernel<<<DimGrid, DimBlock>>>(d_Pin, d_Pout, m, n); // Drives me nuts that they do row major... (y,x) feels so wrong.
}

1.8
===
Quick review of matrix multiplication
Block/Thread index to data index mapping
Loop control flow in kernels

[Row, Col] is generated by taking the inner product of a row with A (using index Row) and column B (with index Col)
A_(m,n)
B_(n,k)
C_(m,k) = AxB

// In CPU Host code
void MatrixMultiplicationOnHost(int m, int n, int k, float* A, float* B, float* C) {
	for (int row = 0; row < m; row++) {
		for (int col = 0; col < k; col++) {
			float sum = 0;
			for (int i = 0; i < n; i++) {
				float a = A[row*n + i];
				float b = B[col + k*i];
				sum += a*b;
			}
			C[row*k + col] = sum;
		}
	}
}

For 

// Assume TILE_WIDTH is a #define constant of 2
__host__
void MatrixMultiplication(int m, int n, int k, float* d_A, float* d_B, float* d_C) {
	dim3 dimGrid((k-1) / TILE_WIDTH + 1, (m-1) / TILE_WIDTH + 1, 1);
	dim3 dimBlock(TILE_WIDTH, TILE_WIDTH, 1);
	KernelMatrixMultiplication<dimGrid, dimBlock>>>(m, n, k, d_A, d_B, d_C);
}

__global void KernelMatrixMultiplication(int m, int n, int k, float* d_A, float* d_B, float* d_C) {
	int row = blockIdx.y * blockDim.y + threadIdx.y;
	int col = blockIdx.x * blockDim.x + threadIdx.x;
	if (row >= m) || (col >= k) {
		return;
	}

	float cValue = 0.0;
	for (int i = 0; i < n; i++) {
		cValue += A[row*n + i] * B[col + i*k];
	}
	C[row*k + col] = cValue;
}


LAB NOTES

Logging
	wbLog takes: OFF, FATAL, ERROR, WARN, INFO, DEBUG, or TRACE
	wbLog(TRACE, "The value of x = ", x)
Timing
	wbTime_start(tag, msg)
	wbTime_stop(tag, msg)
	These tags must match

Local Development
The library used throughout the course can be downloaded from Github (https://github.com/abduld/libwb).
Once linked against the library, you can launch the your program as follows:
	./program -e <expected_output_file> -i <input_file_1>,<input_file_2> -o <output_file> -t <type>
The <expected_output_file> and <input_file_n> are the input and output files provided in the dataset.
The <output_file> is the location youâ€™d like to place the output from your program.
The <type> is the output file type: vector, matrix, or image. If an MP does not expect an input or output, then pass none as the parameter.

Kerrigan (my laptop, Asus G53SX) has a GTX 560M, which supports CUDA 2.1

General flow:
	Allocate device memory
	Copy host memory to device
	Initialize thread block and kernel grid dimensions
	Invoke CUDA kernel
	Copy results from device to host
	Free device memory
	Write the CUDA kernel

Output of Lap Device Output:
Kind	Location		Time (ms)	Message
GPUs	main.cu::14		0.178785	Getting GPU Data.

Level	Location	Message
Trace	main::30	There is 1 device supporting CUDA
Trace	main::36	Device 0 name: GRID K520
Trace	main::37	Computational Capabilities: 3.0
Trace	main::38	Maximum global memory size: 4294770688
Trace	main::39	Maximum constant memory size: 65536
Trace	main::40	Maximum shared memory size per block: 49152
Trace	main::43	Maximum block dimensions: 1024 x 1024 x 64
Trace	main::46	Maximum grid dimensions: 2147483647 x 65535 x 65535
Trace	main::47	Warp size: 32

int main(int argc, char ** argv) {
    wbArg_read(argc, argv);
    int deviceCount;
    cudaGetDeviceCount(&deviceCount);
    wbTime_start(GPU, "Getting GPU Data."); //@@ start a timer
    for (int dev = 0; dev < deviceCount; dev++) {
        cudaDeviceProp deviceProp;
        cudaGetDeviceProperties(&deviceProp, dev);
        if (dev == 0) {
            if (deviceProp.major == 9999 && deviceProp.minor == 9999) {
                wbLog(TRACE, "No CUDA GPU has been detected");
                return -1;
            } else if (deviceCount == 1) {
                //@@ WbLog is a provided logging API (similar to Log4J).
                //@@ The logging function wbLog takes a level which is either
                //@@ OFF, FATAL, ERROR, WARN, INFO, DEBUG, or TRACE and a
                //@@ message to be printed.
                wbLog(TRACE, "There is 1 device supporting CUDA");
            } else {
                wbLog(TRACE, "There are ", deviceCount, " devices supporting CUDA");
            }
        }

        wbLog(TRACE, "Device ", dev, " name: ", deviceProp.name);
        wbLog(TRACE, " Computational Capabilities: ", deviceProp.major, ".", deviceProp.minor);
        wbLog(TRACE, " Maximum global memory size: ", deviceProp.totalGlobalMem);
        wbLog(TRACE, " Maximum constant memory size: ", deviceProp.totalConstMem);
        wbLog(TRACE, " Maximum shared memory size per block: ", deviceProp.sharedMemPerBlock);
        wbLog(TRACE, " Maximum block dimensions: ", deviceProp.maxThreadsDim[0], " x ",
                                                    deviceProp.maxThreadsDim[1], " x ",
                                                    deviceProp.maxThreadsDim[2]);
        wbLog(TRACE, " Maximum grid dimensions: ", deviceProp.maxGridSize[0], " x ",
                                                   deviceProp.maxGridSize[1], " x ",
                                                   deviceProp.maxGridSize[2]);
        wbLog(TRACE, " Warp size: ", deviceProp.warpSize);
    }
    wbTime_stop(GPU, "Getting GPU Data."); //@@ stop the timer
    return 0;
}